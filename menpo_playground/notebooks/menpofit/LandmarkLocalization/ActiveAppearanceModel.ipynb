{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Appearance Models - Advanced\n",
    "The aim of this notebook is to showcase some of the more advanced features that are available for building and fitting AAMs using ``menpo`` and ``menpofit``. \n",
    "\n",
    "Note that this notebook assumes that the user has previously gone through the AAMs Basic notebook and he/she is already familiar with the basic AAMs and `Menpo` concepts explained in there. The AAMs Basic notebook shows us how through the use of the `AAMBuilder` and the `LucasKanadeAAMFitter` classes one can easily build and fit basic AAMs using `Menpo`. In reality, `Menpo`'s framework for AAMs is a lot more powerful and we will proceed to show how a large variety of different AAMs and different fitting algorithms can be used by simply specifying the right keyword arguments on the two previous classes.\n",
    "\n",
    "The complete list of the available keyword arguments for the `AAMBuilder` and their corresponding detailed explanations can be found in Menpo's documentation. Remember that their documentation can be checked at any time from the IPython Notebook by simply running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpofit.aam import HolisticAAM\n",
    "\n",
    "HolisticAAM??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is structured in several short sections, each one explaining a different advanced concept related to `Menpo`'s framework for building and fitting AAMs. Specifically:\n",
    "\n",
    "1. Loading data\n",
    "2. Building and fitting AAMs with user-defined appearance features\n",
    "3. Fitting AAMs using different `LucasKanade` based algorithms.\n",
    "4. Patch-Based AAM building and fitting\n",
    "5. Fitting AAMs using `Menpo's` Supervised Descent (SD) framework\n",
    "\n",
    "Note that, in general, these sections are fairly detached from one another and the order in which they are listed bellow is not specifically relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the following sections will rely on some training and testing data to build and fit their respective AAMs. Once again, we will rely on the training and test sets of the LFPW database for this purpose.\n",
    "\n",
    "Note that the necessary steps required for acquiring the LFPW database are explained in detail in the AAMs Basics notebook and the user is simply referred to that notebook for this matter.\n",
    "\n",
    "Let us define the function `load_database()` for loading a set of images, cropping them and converting them to greyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import menpo.io as mio\n",
    "\n",
    "# method to load a database\n",
    "def load_database(path_to_images, crop_percentage, max_images=None):\n",
    "    images = []\n",
    "    # load landmarked images\n",
    "    for i in mio.import_images(path_to_images, max_images=max_images, verbose=True):\n",
    "        # crop image\n",
    "        i = i.crop_to_landmarks_proportion(crop_percentage)\n",
    "        \n",
    "        # convert it to grayscale if needed\n",
    "        if i.n_channels == 3:\n",
    "            i = i.as_greyscale(mode='luminosity')\n",
    "            \n",
    "        # append it to the list\n",
    "        images.append(i)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also define the path to the LFPW database.\n",
    "\n",
    "Note that the necessary steps required for acquiring the LFPW dataset used throughout this notebook were previously explained in the AAMs Basics notebook and we simply refer the user to that notebook for this matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_lfpw = Path('/vol/atlas/databases/lfpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and visualize the __training__ images with a crop proportion of 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "/vol/atlas/databases/lfpw/trainset is an invalid glob and not a dir",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dd1eb4028331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_lfpw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'trainset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-36c84d65198d>\u001b[0m in \u001b[0;36mload_database\u001b[0;34m(path_to_images, crop_percentage, max_images)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# load landmarked images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# crop image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_to_landmarks_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_percentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lee/Desktop/Menpo_Playground_With_Mynotes/menpo_playground/src/lib/python3.5/site-packages/menpo/io/input/base.py\u001b[0m in \u001b[0;36mimport_images\u001b[0;34m(pattern, max_images, shuffle, landmark_resolver, normalize, normalise, as_generator, verbose)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mas_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mimporter_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     )\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lee/Desktop/Menpo_Playground_With_Mynotes/menpo_playground/src/lib/python3.5/site-packages/menpo/io/input/base.py\u001b[0m in \u001b[0;36m_import_glob_lazy_list\u001b[0;34m(pattern, extension_map, max_assets, landmark_resolver, shuffle, as_generator, landmark_ext_map, landmark_attach_func, importer_kwargs, verbose)\u001b[0m\n\u001b[1;32m    682\u001b[0m                            verbose=False):\n\u001b[1;32m    683\u001b[0m     filepaths = list(glob_with_suffix(pattern, extension_map,\n\u001b[0;32m--> 684\u001b[0;31m                                       sort=(not shuffle)))\n\u001b[0m\u001b[1;32m    685\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lee/Desktop/Menpo_Playground_With_Mynotes/menpo_playground/src/lib/python3.5/site-packages/menpo/io/input/base.py\u001b[0m in \u001b[0;36mglob_with_suffix\u001b[0;34m(pattern, extensions_map, sort)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mfilepaths\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \"\"\"\n\u001b[0;32m--> 899\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pathlib_glob_for_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0mpossible_exts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_possible_extensions_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions_map\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossible_exts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lee/Desktop/Menpo_Playground_With_Mynotes/menpo_playground/src/lib/python3.5/site-packages/menpo/io/input/base.py\u001b[0m in \u001b[0;36m_pathlib_glob_for_pattern\u001b[0;34m(pattern, sort)\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             raise ValueError('{} is an invalid glob and '\n\u001b[0;32m--> 861\u001b[0;31m                              'not a dir'.format(pattern))\n\u001b[0m\u001b[1;32m    862\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0mpreglob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgsplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: /vol/atlas/databases/lfpw/trainset is an invalid glob and not a dir"
     ]
    }
   ],
   "source": [
    "training_images = load_database(path_to_lfpw / 'trainset', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpowidgets import visualize_images\n",
    "\n",
    "visualize_images(training_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and visualize 5 __test__ images with a crop proportion of 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images = load_database(path_to_lfpw / 'testset', 0.5, max_images=5)\n",
    "print（path_to_lfpw / 'testset'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_images(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building and fitting AAMs with user-defined appearance features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful and general characteristics of the `AAMBuilder` class is that it allows the user to build AAMs with arbitrarily user-defined appearance features. Instead of restricting the user to a set of predefined appearance features, `Menpo` allows him/her to define new features (perhaps simple combinations of existing image features supported by Menpo or perhaps completely new (even discriminative!) features designed by the user) and pass them to the `AAMbuilder`.\n",
    "\n",
    "All that is required is that the user decorates his/her desired feature computation using one of the feature decorators defined in Menpo, i.e. `@ndfeature`, `@imgfeature` or `@winitfeature`. The two first decorators allow the user to define feature computations in terms of simple `numpy` arrays and `Menpo` images. The third decorator supports the definition of features that rely on a cell/block structure (like **HOG** or **SIFT**). Note that, the only computation that needs to be defined by the user defined feature funtion is the pure feature computation over the image pixels; landmarks and mask attached to the image are handled by the previous decorators.\n",
    "\n",
    "The next cell shows a simple example of such a function, in which **igo** features are computed on an feature image that was already obtained by computing **igo** features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpo.feature import imgfeature, igo\n",
    "\n",
    "@imgfeature\n",
    "def custom_double_igo(image):\n",
    "    return igo(igo(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_double_igo(training_images[0]).view();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build an AAM using our new feature representation with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpofit.aam import HolisticAAM\n",
    "\n",
    "\n",
    "aam = HolisticAAM(\n",
    "    training_images,\n",
    "    group='PTS',\n",
    "    verbose=True,\n",
    "    holistic_features=custom_double_igo, \n",
    "    diagonal=120, \n",
    "    scales=(0.5, 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(aam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualize the AAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aam.view_aam_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained AAM can be used to fit the loaded testing images using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpofit.aam import LucasKanadeAAMFitter\n",
    "\n",
    "fitter = LucasKanadeAAMFitter(aam, n_shape=[6, 12], n_appearance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpofit.fitter import noisy_shape_from_bounding_box\n",
    "\n",
    "fitting_results = []\n",
    "\n",
    "for i in test_images:\n",
    "    # obtain original landmarks\n",
    "    gt_s = i.landmarks['PTS'].lms\n",
    "    \n",
    "    # generate perturbed landmarks\n",
    "    s = noisy_shape_from_bounding_box(gt_s, gt_s.bounding_box())\n",
    "    \n",
    "    # fit image\n",
    "    fr = fitter.fit_from_shape(i, s, gt_shape=gt_s) \n",
    "    fitting_results.append(fr)\n",
    "    \n",
    "    print(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the fitting results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpowidgets import visualize_fitting_result\n",
    "\n",
    "visualize_fitting_result(fitting_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Patch-Based AAM building and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the previous AAM examples were using a holistic appearance representation, i.e. the whole face was employed as an appearance vector. Herein, we show how to build and fit a patch-based AAM. This means that the appearance representation consists of small patches extracted around each of the landmark points.\n",
    "\n",
    "The patch-based AAM builder, `PatchAAM`, has the same parameters as the `HolisticAAM`, with the addition of the `patch_shape` parameter. We'll train such an AAM using IGOs with double angles as feature representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpofit.aam import PatchAAM\n",
    "from menpo.feature import double_igo\n",
    "\n",
    "patch_based_aam = PatchAAM(\n",
    "    training_images,\n",
    "    group='PTS',\n",
    "    verbose=True,\n",
    "    holistic_features=double_igo, \n",
    "    diagonal=120, \n",
    "    scales=(0.5, 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(patch_based_aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch_based_aam.view_aam_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting of the trained Patch-Based AAM can use any of the algorithms of the `LucasKanadeAAMFitter` presented in the previous section as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitter = LucasKanadeAAMFitter(patch_based_aam, \n",
    "                              n_shape=[3, 12], \n",
    "                              n_appearance=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menpofit.fitter import noisy_shape_from_bounding_box\n",
    "\n",
    "fitting_results = []\n",
    "\n",
    "for i in test_images:\n",
    "    # obtain original landmarks\n",
    "    gt_s = i.landmarks['PTS'].lms\n",
    "    \n",
    "    # generate perturbed landmarks\n",
    "    s = noisy_shape_from_bounding_box(gt_s, gt_s.bounding_box())\n",
    "    \n",
    "    # fit image\n",
    "    fr = fitter.fit_from_shape(i, s, gt_shape=gt_s) \n",
    "    fitting_results.append(fr)\n",
    "    \n",
    "    print(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_fitting_result(fitting_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
